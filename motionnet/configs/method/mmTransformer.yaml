# common
model_name: mmTransformer
num_modes: 6
hidden_size: 64
num_encoder_layers: 4
num_decoder_layers: 3
tx_hidden_size: 384
tx_num_heads: 16
dropout: 0.1
entropy_weight: 40.0
kl_weight: 20.0
residual: 0.5
use_FDEADE_aux_loss: True

# train
max_epochs: 200
learning_rate: 0.0009
learning_rate_sched: [10, 20, 30, 40, 50]
optimizer: Adam 
scheduler: multistep 
ewc_lambda: 2000
train_batch_size: 128 
eval_batch_size: 256
grad_clip_norm: 5

# data related
max_num_agents: 15
map_range: 100
max_num_roads: 256
max_points_per_lane: 10 #20
manually_split_lane: False
point_sampled_interval: 1
num_points_each_polyline: 10 #20
vector_break_dist_thresh: 1.0

#mmTransformer
in_channels: 4
lane_channels: 7
out_channels: 120
K: 6
increasetime: 3
queries: 6
num_guesses: 6
queries_dim: 64
enc_dim: 64
aux_task: False
subgraph_width: 32
num_subgraph_layres: 2
lane_length: 10
history_num_frames: 20
future_num_frames: 60